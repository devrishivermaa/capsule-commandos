{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "import timm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10  \n",
    "batch_size = 32\n",
    "learning_rate = 1e-4\n",
    "epochs = 20\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    #transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    #transforms.RandomAutocontrast(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(\n",
    "    root='Dataset/training',\n",
    "    transform=transform_train\n",
    ")\n",
    "\n",
    "val_dataset = datasets.ImageFolder(\n",
    "    root='Dataset/validation',\n",
    "    transform=transform_val\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arpan/anaconda3/envs/opencv-py3/lib/python3.11/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('davit_small', pretrained=True, num_classes=num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, save_dir='models_new'):\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "     \n",
    "     \n",
    "        validate_model(model, val_loader, criterion)\n",
    "\n",
    "        \n",
    "        model_path = os.path.join(save_dir, f'model_epoch_{epoch+1}.pth')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f'Model saved to {model_path}')\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    print(f'Validation Loss: {val_loss/len(val_loader):.4f}, Accuracy: {100 * correct / total:.4f}%') \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 0.3731, Accuracy: 88.63%\n",
      "Validation Loss: 0.2484, Accuracy: 92.2762%\n",
      "Model saved to models_davit/model_epoch_1.pth\n",
      "Epoch [2/20], Loss: 0.1982, Accuracy: 93.52%\n",
      "Validation Loss: 0.2004, Accuracy: 93.4788%\n",
      "Model saved to models_davit/model_epoch_2.pth\n",
      "Epoch [3/20], Loss: 0.1523, Accuracy: 94.96%\n",
      "Validation Loss: 0.1784, Accuracy: 94.1049%\n",
      "Model saved to models_davit/model_epoch_3.pth\n",
      "Epoch [4/20], Loss: 0.1255, Accuracy: 95.79%\n",
      "Validation Loss: 0.1816, Accuracy: 94.1607%\n",
      "Model saved to models_davit/model_epoch_4.pth\n",
      "Epoch [5/20], Loss: 0.1032, Accuracy: 96.46%\n",
      "Validation Loss: 0.1667, Accuracy: 94.3962%\n",
      "Model saved to models_davit/model_epoch_5.pth\n",
      "Epoch [6/20], Loss: 0.0878, Accuracy: 96.92%\n",
      "Validation Loss: 0.1893, Accuracy: 94.0739%\n",
      "Model saved to models_davit/model_epoch_6.pth\n",
      "Epoch [7/20], Loss: 0.0767, Accuracy: 97.30%\n",
      "Validation Loss: 0.2044, Accuracy: 94.1793%\n",
      "Model saved to models_davit/model_epoch_7.pth\n",
      "Epoch [8/20], Loss: 0.0693, Accuracy: 97.52%\n",
      "Validation Loss: 0.1878, Accuracy: 94.5016%\n",
      "Model saved to models_davit/model_epoch_8.pth\n",
      "Epoch [9/20], Loss: 0.0572, Accuracy: 97.86%\n",
      "Validation Loss: 0.1992, Accuracy: 94.4334%\n",
      "Model saved to models_davit/model_epoch_9.pth\n",
      "Epoch [10/20], Loss: 0.0531, Accuracy: 98.06%\n",
      "Validation Loss: 0.1831, Accuracy: 94.5326%\n",
      "Model saved to models_davit/model_epoch_10.pth\n",
      "Epoch [11/20], Loss: 0.0493, Accuracy: 98.14%\n",
      "Validation Loss: 0.1871, Accuracy: 94.5450%\n",
      "Model saved to models_davit/model_epoch_11.pth\n",
      "Epoch [12/20], Loss: 0.0472, Accuracy: 98.21%\n",
      "Validation Loss: 0.2285, Accuracy: 94.0429%\n",
      "Model saved to models_davit/model_epoch_12.pth\n",
      "Epoch [13/20], Loss: 0.0397, Accuracy: 98.49%\n",
      "Validation Loss: 0.2253, Accuracy: 94.3466%\n",
      "Model saved to models_davit/model_epoch_13.pth\n",
      "Epoch [14/20], Loss: 0.0417, Accuracy: 98.33%\n",
      "Validation Loss: 0.2095, Accuracy: 94.4892%\n",
      "Model saved to models_davit/model_epoch_14.pth\n",
      "Epoch [15/20], Loss: 0.0404, Accuracy: 98.41%\n",
      "Validation Loss: 0.2099, Accuracy: 94.2351%\n",
      "Model saved to models_davit/model_epoch_15.pth\n",
      "Epoch [16/20], Loss: 0.0346, Accuracy: 98.64%\n",
      "Validation Loss: 0.2334, Accuracy: 94.5140%\n",
      "Model saved to models_davit/model_epoch_16.pth\n",
      "Epoch [17/20], Loss: 0.0364, Accuracy: 98.51%\n",
      "Validation Loss: 0.2327, Accuracy: 94.5636%\n",
      "Model saved to models_davit/model_epoch_17.pth\n",
      "Epoch [18/20], Loss: 0.0347, Accuracy: 98.56%\n",
      "Validation Loss: 0.2275, Accuracy: 94.2351%\n",
      "Model saved to models_davit/model_epoch_18.pth\n",
      "Epoch [19/20], Loss: 0.0304, Accuracy: 98.73%\n",
      "Validation Loss: 0.2284, Accuracy: 94.3838%\n",
      "Model saved to models_davit/model_epoch_19.pth\n",
      "Epoch [20/20], Loss: 0.0315, Accuracy: 98.75%\n",
      "Validation Loss: 0.2240, Accuracy: 94.8673%\n",
      "Model saved to models_davit/model_epoch_20.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs, save_dir='models_davit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
